{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-prediction",
   "metadata": {},
   "source": [
    "# Visualisation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import torch \n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from labml import lab, tracker, experiment, monit\n",
    "from labml.configs import BaseConfigs, option\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-vault",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Below are not relevant for training purpose.   \n",
    "\n",
    "# def normalize_tensor(tensor, value_range=None):\n",
    "#     tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "#     if value_range is not None and not isinstance(value_range, tuple):\n",
    "#         raise TypeError(\"value_range has to be a tuple (min, max) if specified. min and max are numbers\")\n",
    "\n",
    "#     def norm_ip(img, low, high):\n",
    "#         img.clamp_(min=low, max=high)\n",
    "#         img.sub_(low).div_(max(high - low, 1e-5))\n",
    "\n",
    "#     def norm_range(t, value_range):\n",
    "#         if value_range is not None:\n",
    "#             norm_ip(t, value_range[0], value_range[1])\n",
    "#         else:\n",
    "#             norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "#     for t in tensor:\n",
    "#         norm_range(t, value_range)\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "# tensor = torch.load('samples.pt')\n",
    "# norm_tensor = normalize_tensor(tensor)\n",
    "# torch.save(norm_tensor, 'samples.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-jumping",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.load('samples_bbb.pt')\n",
    "# s_tensor = torch.cat([tensor] * 30, dim=0)\n",
    "# print(s_tensor.shape)\n",
    "# torch.save(s_tensor, 'l_samples.pt')\n",
    "print(tensor.shape)\n",
    "print(torch.max(tensor))\n",
    "\n",
    "tensor = torch.load('dataset.pt')\n",
    "print(tensor.shape)\n",
    "print(torch.max(tensor))\n",
    "\n",
    "# # Below are not relevant for training purpose.   \n",
    "\n",
    "# def normalize_tensor(tensor, value_range=None):\n",
    "#     tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "#     if value_range is not None and not isinstance(value_range, tuple):\n",
    "#         raise TypeError(\"value_range has to be a tuple (min, max) if specified. min and max are numbers\")\n",
    "\n",
    "#     def norm_ip(img, low, high):\n",
    "#         img.clamp_(min=low, max=high)\n",
    "#         img.sub_(low).div_(max(high - low, 1e-5))\n",
    "\n",
    "#     def norm_range(t, value_range):\n",
    "#         if value_range is not None:\n",
    "#             norm_ip(t, value_range[0], value_range[1])\n",
    "#         else:\n",
    "#             norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "#     for t in tensor:\n",
    "#         norm_range(t, value_range)\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "# tensor = torch.load('samples_bbb.pt')\n",
    "# norm_tensor = normalize_tensor(tensor)\n",
    "# torch.save(norm_tensor, 'samples_bbb.pt')\n",
    "\n",
    "\n",
    "# tensor_n = tensor[0:16, :, :, :]\n",
    "# print(tensor_n.shape)\n",
    "# torch.save(tensor_n, 'tensor_n.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-bearing",
   "metadata": {},
   "source": [
    "# Compare gen samples with true samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are not relevant for training purpose.   \n",
    "# Display the sampled images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tensor = torch.load('samples.pt')\n",
    "print('gen_shape:', tensor.shape)\n",
    "N = tensor.size(0)\n",
    "\n",
    "# Create a grid of images from the tensor\n",
    "grid = torchvision.utils.make_grid(tensor, nrow=int(N**(0.5)), padding=1, normalize=True, pad_value=1.0, scale_each=True)\n",
    "\n",
    "# Convert the grid to a numpy array and transpose the dimensions to match the expected format by matplotlib\n",
    "grid = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Show the grid of images using matplotlib\n",
    "plt.imshow(grid)\n",
    "plt.axis('off')\n",
    "plt.savefig('gen_samples.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # display the true images\n",
    "tensor = torch.load('true_samples.pt')\n",
    "print('true_shape:', tensor.shape)\n",
    "N = tensor.size(0)\n",
    "\n",
    "# Create a grid of images from the tensor\n",
    "grid = torchvision.utils.make_grid(tensor, nrow=int(N**(0.5)), padding=1, normalize=True, pad_value=1.0)\n",
    "\n",
    "# Convert the grid to a numpy array and transpose the dimensions to match the expected format by matplotlib\n",
    "grid = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Show the grid of images using matplotlib\n",
    "plt.imshow(grid)\n",
    "plt.axis('off')\n",
    "plt.savefig('true_samples.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Compare two plots\n",
    "# Load the saved images\n",
    "gen_samples = plt.imread('gen_samples.png')\n",
    "true_samples = plt.imread('true_samples.png')\n",
    "\n",
    "# Create a new figure with two subplots side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 9), facecolor='white')\n",
    "\n",
    "# Plot the generated samples on the left subplot\n",
    "axs[0].imshow(gen_samples)\n",
    "axs[0].set_title('Generated Samples', fontsize=20)\n",
    "axs[0].axis('off')\n",
    "\n",
    "\n",
    "# Plot the true samples on the right subplot\n",
    "axs[1].imshow(true_samples)\n",
    "axs[1].set_title('True Samples', fontsize=20)\n",
    "axs[1].axis('off')\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "# Display the figure\n",
    "plt.savefig('Gen_vs_True_CIFAR10.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-friday",
   "metadata": {},
   "source": [
    "# Compute Inception Score & FID score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-allergy",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# import sys\n",
    "# original_path = sys.path.copy()  # make a copy of the original sys.path\n",
    "\n",
    "# # remove the system-wide dist-packages directory from sys.path\n",
    "# sys.path = ['/home/research/yuqijing/sem2project/Diffusion',\n",
    "#     '/home/research/yuqijing/.local/lib/python3.9/site-packages/git/ext/gitdb',\n",
    "#     '/usr/lib/python39.zip',\n",
    "#     '/usr/lib/python3.9',\n",
    "#     '/usr/lib/python3.9/lib-dynload',\n",
    "#     '',\n",
    "#     '/home/research/yuqijing/.local/lib/python3.9/site-packages',\n",
    "#     '/usr/local/lib/python3.9/dist-packages',\n",
    "#     '/usr/lib/python3/dist-packages',\n",
    "#     '/usr/lib/python3/dist-packages/IPython/extensions',\n",
    "#     '/home/research/yuqijing/.ipython']\n",
    "\n",
    "# from torchmetrics.image.inception import InceptionScore\n",
    "# # from /home/research/yuqijing/.local/lib/python3.9/site-packages import torchmetrics\n",
    "\n",
    "\n",
    "# print(sys.path)\n",
    "\n",
    "\n",
    "\n",
    "# inception = InceptionScore(normalize=True)\n",
    "# # load images\n",
    "# imgs = torch.load('samples.pt')\n",
    "# imgs = imgs.cpu()\n",
    "# inception.update(imgs)\n",
    "# inception.compute()\n",
    "# print(inception.compute())\n",
    "\n",
    "# # add the system-wide dist-packages directory back to sys.path\n",
    "# sys.path = original_path  # restore sys.path to its original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-effects",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "original_path = sys.path.copy()  # make a copy of the original sys.path\n",
    "\n",
    "# remove the system-wide dist-packages directory from sys.path\n",
    "sys.path = ['/home/research/yuqijing/sem2project/Diffusion',\n",
    "    '/home/research/yuqijing/.local/lib/python3.9/site-packages/git/ext/gitdb',\n",
    "    '/usr/lib/python39.zip',\n",
    "    '/usr/lib/python3.9',\n",
    "    '/usr/lib/python3.9/lib-dynload',\n",
    "    '',\n",
    "    '/home/research/yuqijing/.local/lib/python3.9/site-packages',\n",
    "    '/usr/local/lib/python3.9/dist-packages',\n",
    "    '/usr/lib/python3/dist-packages',\n",
    "    '/usr/lib/python3/dist-packages/IPython/extensions',\n",
    "    '/home/research/yuqijing/.ipython']\n",
    "\n",
    "### Inception Score\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "# from /home/research/yuqijing/.local/lib/python3.9/site-packages import torchmetrics\n",
    "start_time = time.time()\n",
    "inception = InceptionScore(feature=64, normalize=True)\n",
    "# load images\n",
    "imgs = torch.load('data_small.pt')\n",
    "imgs = imgs.cpu()\n",
    "inception.update(imgs)\n",
    "inception.compute()\n",
    "end_time = time.time()\n",
    "elp_time = end_time - start_time\n",
    "print(inception.compute())\n",
    "print('compute time:', elp_time, 'seconds')\n",
    "\n",
    "# ### FID\n",
    "# from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "# start_time = time.time()\n",
    "# fid = FrechetInceptionDistance(feature=2048, normalize=True)\n",
    "# # load images\n",
    "# imgs_1 = torch.load('dataset.pt')\n",
    "# imgs_1 = imgs_1.cpu()\n",
    "# imgs_2 = torch.load('samples_bbb.pt')\n",
    "# print(torch.max(imgs_1))\n",
    "# imgs_2 = imgs_2.cpu()\n",
    "# imgs_dist1 = imgs_1\n",
    "# imgs_dist2 = imgs_2\n",
    "# fid.update(imgs_dist1, real=True)\n",
    "# fid.update(imgs_dist2, real=False)\n",
    "# fid.compute()\n",
    "# end_time = time.time()\n",
    "# elp_time = end_time - start_time\n",
    "# print(fid.compute())\n",
    "# print('compute time:', elp_time, 'seconds')\n",
    "\n",
    "# for reference, the true cifar10 achieves IS of 11.24 +/- 0.12\n",
    "# for reference, the best generative model (score-based + diffusion model) achieves FID of 1.64 (lowest).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add the system-wide dist-packages directory back to sys.path\n",
    "sys.path = original_path  # restore sys.path to its original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID & IS for CIFAR10 dataset\n",
    "\n",
    "import torch\n",
    "from pytorch_gan_metrics import get_inception_score, get_fid\n",
    "\n",
    "images = torch.load('/home/research/yuqijing/sem2project/Diffusion/FID_statistics/data_small.pt')\n",
    "print(images.shape)\n",
    "print(torch.max(images))\n",
    "# IS calculation\n",
    "IS, IS_std = get_inception_score(images)\n",
    "print(IS, IS_std)\n",
    "# FID calculation\n",
    "FID = get_fid(images, './FID_statistics/statistics_c.npz') # Frechet Inception Distance\n",
    "print(FID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-toyota",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 1 channel images to 3 channel images for MNIST images.\n",
    "images = torch.load('samples_bbb.pt')\n",
    "print(images.shape)\n",
    "print(images[3, :, 10, 10])\n",
    "images_2 = torch.load('true_samples_bbb.pt')\n",
    "print(images_2.shape)\n",
    "\n",
    "images_3 = torch.cat([images, images, images], dim=1)\n",
    "print(images_3.shape)\n",
    "print(images_3[3, :, 10, 10])\n",
    "images_4 = torch.cat([images_2, images_2, images_2], dim=1)\n",
    "print(images_4.shape)\n",
    "torch.save(images_4, 'true_samples_m.pt')\n",
    "torch.save(images_3, 'samples_m.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-electric",
   "metadata": {},
   "source": [
    "# IS for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IS for the MNIST images. \n",
    "import torch\n",
    "from pytorch_gan_metrics import get_inception_score, get_fid\n",
    "\n",
    "images = torch.load('true_samples_m.pt')\n",
    "print(images.shape)\n",
    "print(torch.max(images))\n",
    "# IS calculation\n",
    "IS, IS_std = get_inception_score(images)\n",
    "print(IS, IS_std)\n",
    "\n",
    "# IS for the 10_000 true MNIST images are 2.004\n",
    "# IS for the 10_000 generated MNIST images are 1.8745\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-airline",
   "metadata": {},
   "source": [
    "# FID for MNIST please refer to stf in sem2project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-rubber",
   "metadata": {
    "tags": []
   },
   "source": [
    "# For creating reference dataset dataset.pt, 10,000, 3, 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-maldives",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # tensor = np.load('cifar10-32x32.npz')\n",
    "\n",
    "# # print(tensor.files)\n",
    "# tensor = torch.load('samples.pt')\n",
    "# tensor.shape\n",
    "\n",
    "# Sample Images from the true density\n",
    "# Sample Images from the true density\n",
    "import random\n",
    "N = 10_000\n",
    "    \n",
    "# Define the transform to apply to each image\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 dataset with the defined transform\n",
    "dataset_f = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Get the indices of 16 randomly sampled images from the dataset\n",
    "indices = random.sample(range(len(dataset_f)), N)\n",
    "\n",
    "# Create a tensor to hold the 16 images\n",
    "tensor = torch.zeros((N, 3, 32, 32))\n",
    "\n",
    "# Load each sampled image into the tensor\n",
    "for i, idx in enumerate(indices):\n",
    "    image, _ = dataset_f[idx]\n",
    "    tensor[i] = image\n",
    "\n",
    "torch.save(tensor, 'dataset.pt')\n",
    "\n",
    "# tensor = torch.load('dataset.pt')\n",
    "# print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-pleasure",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Drawing samples from dataset.pt which is 10,000 true images from CIFAR10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-mauritius",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # tensor = np.load('cifar10-32x32.npz')\n",
    "\n",
    "# # print(tensor.files)\n",
    "# tensor = torch.load('samples.pt')\n",
    "# tensor.shape\n",
    "\n",
    "# Sample Images from the true density\n",
    "# Sample Images from the true density\n",
    "import random\n",
    "N = 1000\n",
    "    \n",
    "# Define the transform to apply to each image\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 dataset with the defined transform\n",
    "dataset_f = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Get the indices of 16 randomly sampled images from the dataset\n",
    "indices = random.sample(range(len(dataset_f)), N)\n",
    "\n",
    "# Create a tensor to hold the 16 images\n",
    "tensor = torch.zeros((N, 3, 32, 32))\n",
    "\n",
    "# Load each sampled image into the tensor\n",
    "for i, idx in enumerate(indices):\n",
    "    image, _ = dataset_f[idx]\n",
    "    tensor[i] = image\n",
    "\n",
    "torch.save(tensor, 'data_small.pt')   # 'data_small.pt' is a set of images of the form of tensor [N, 3, 32, 32]\n",
    "\n",
    "# tensor = torch.load('dataset.pt')\n",
    "# print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-costa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert .pt to 10,000 number of images in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-colors",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Set the name of the new directory\n",
    "directory_name = \"images_true\"\n",
    "\n",
    "# Create the new directory if it doesn't already exist\n",
    "if not os.path.exists(directory_name):\n",
    "    os.mkdir(directory_name)\n",
    "    \n",
    "\n",
    "tensor = torch.load('true_samples_m.pt')\n",
    "print('samples:', tensor.shape)\n",
    "N = tensor.size(0)\n",
    "print(N)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10_000):\n",
    "    \n",
    "    image = tensor[i]\n",
    "    \n",
    "    # Set the path where the new image file will be saved\n",
    "    new_image_path = os.path.join(directory_name, f\"image {i}.jpg\")\n",
    "\n",
    "    # Save the image to the new directory\n",
    "    save_image(image, new_image_path)\n",
    "\n",
    "# for i in range()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-treasure",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "5.719s == 2048, 2.7s == 192, 5.5s == no, 4.8s == 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-muslim",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Install packages in specific path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-bread",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #!pip install --target=/home/local/lib/python3.9/dist-packages/ torch-fidelity\n",
    "# # !pip install --no-deps --target=/home/research/yuqijing/.local/lib/python3.9/site-packages torchmetrics\n",
    "# !pip install --no-deps --target=/home/research/yuqijing/sem2project/Diffusion torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-klein",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
